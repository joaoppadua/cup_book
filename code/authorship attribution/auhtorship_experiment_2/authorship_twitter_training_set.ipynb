{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to organize training data for analysis authorship of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle files\n",
    "\n",
    "with open('./data/twitter_train.pkl', 'rb') as f:\n",
    "    twitter_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two large concatenated train sets\n",
    "\n",
    "marker = \"\\n---TWEET---\\n\"\n",
    "author_texts = {}\n",
    "\n",
    "for author in twitter_train['author'].unique():\n",
    "    tweets = twitter_train[twitter_train['author'] == author]['content'].astype(str)\n",
    "    author_texts[author] = marker.join(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: rihanna\n",
      "‚Äú@RihrihFreak: I knew Rih would RT that..didn't she get slap for doing that to her late gran? Lol‚Äù first thing that popped in my head bruh\n",
      "---TWEET---\n",
      "Bitchez be like....\"#nomakeup \" http://t.co/6juBO\n",
      "\n",
      "\n",
      "Author: katyperry\n",
      "Amazon prime and slime or nah?\n",
      "---TWEET---\n",
      "@iammelissalynn happy birthday qewtie!!! üòòüòòüòò\n",
      "---TWEET---\n",
      "U also made Kitty Purry happy. ‚Äú@essesfadaperry: @katyperry Ready! downloaded! I am now happy and ro\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the first few tweets for each author\n",
    "for author, text in list(author_texts.items())[:5]:\n",
    "    print(f\"Author: {author}\")\n",
    "    print(text[:200]) \n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: rihanna, Number of tokens: 69587\n",
      "Author: katyperry, Number of tokens: 75595\n"
     ]
    }
   ],
   "source": [
    "# Check the number of tokens in the training set\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "for author, text in author_texts.items():\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"Author: {author}, Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author\n",
      "katyperry    2339\n",
      "rihanna      2301\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of tweets in the training dataset\n",
    "tweet_counts = twitter_train['author'].value_counts()\n",
    "print(tweet_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of the training dataset to use in the prompt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "marker = \"\\n---TWEET---\\n\"\n",
    "sampled_author_texts = {}\n",
    "\n",
    "for author in twitter_train['author'].unique():\n",
    "    author_tweets = twitter_train[twitter_train['author'] == author]['content'].astype(str)\n",
    "    n_total = len(author_tweets)\n",
    "    n_sample = max(1, int(np.ceil(n_total / 3)))  # at least 1 tweet\n",
    "    sampled_tweets = author_tweets.sample(n=n_sample, random_state=42)  # set random_state for reproducibility\n",
    "    sampled_author_texts[author] = marker.join(sampled_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: rihanna, Number of tokens: 23499\n",
      "Author: katyperry, Number of tokens: 24853\n"
     ]
    }
   ],
   "source": [
    "# Check the number of tokens in the sampled dataset\n",
    "for author, text in sampled_author_texts.items():\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"Author: {author}, Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save\n",
    "with open('./data/author_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(author_texts, f)\n",
    "\n",
    "with open('./data/sampled_author_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(sampled_author_texts, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
