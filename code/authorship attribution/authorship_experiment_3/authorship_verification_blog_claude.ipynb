{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship verification experiment using the Kaggle Blog Corpus with Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from credentials import get_credentials_claude\n",
    "\n",
    "API_KEY = get_credentials_claude()\n",
    "\n",
    "client = Anthropic(api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test sets\n",
    "train_path = \"./data/authorship_verification_kaggle_data_2_train.pkl\"\n",
    "test_path = \"./data/authorship_verification_kaggle_data_2_test.pkl\"\n",
    "\n",
    "df_train = pd.read_pickle(train_path)\n",
    "df_test = pd.read_pickle(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training string\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get unique author IDs\n",
    "author_ids = df_train['id'].unique()\n",
    "\n",
    "# Dictionary to store concatenated texts\n",
    "author_samples = {}\n",
    "\n",
    "for author_id in author_ids:\n",
    "    # Sample 30 texts for this author\n",
    "    sample_texts = df_train[df_train['id'] == author_id]['text'].sample(n=35, random_state=42)\n",
    "    # Concatenate into a single string (separated by newlines or spaces)\n",
    "    concatenated = \"\\n\\n\".join(sample_texts)\n",
    "    author_samples[author_id] = concatenated\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: 734562\n",
      "           nah, he's still has that huge ass poster in his bedroom.   and 'orthographical'?  wow, big word.          \n",
      "\n",
      "           when downloading porn just isn't enough anymore:  urlLink imaginarygirlfriends.com          \n",
      "\n",
      "           33% MORE!         \n",
      "\n",
      "           You must all think of me on Sunday, October 27th. It is my 29th birthday. I will probably be unable to attend the Blog-a-bration but will keep you, um, posted.         \n",
      "\n",
      "           Bless your heart, Mir...first Clark, now Edwards...no\n",
      "Author: 449628\n",
      "        Letters of Recommendation   Letters of recommendation are something that you could put in a professional portfolio, for example.  They are often requested in academic settings.  I found some links that hopefully will help  Michael Ernst, an Assistant Professor at MIT, offers his advice:   urlLink Requesting a letter of recommendation   urlLink Writing a letter of recommendation   Monster.com also has some advice:  urlLink Writing a Letter of Recommendation   As an employer, coworker, or \n"
     ]
    }
   ],
   "source": [
    "# Inspect the data\n",
    "\n",
    "for id in author_ids:\n",
    "    print(f\"Author: {id}\")\n",
    "    print(author_samples[id][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random text from the test sample and get its author id\n",
    "test_text = df_test['text'].sample(n=1, random_state=42).values[0]\n",
    "test_author_id = df_test[df_test['text'] == test_text]['id'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Well, you could practice both at the same time...... Provided that the one you are dating is the one you are married to as well.          \n",
      "734562\n"
     ]
    }
   ],
   "source": [
    "print(test_text)\n",
    "print(test_author_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the template prompt\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an expert in linguistic analysis and authorship attribution. Your task is to analyze authorship--i.e. idiolectal--markers that the \\\n",
    "texts reveal about their authors.\n",
    "\n",
    "\n",
    "You will be provided with:\n",
    "1. Training samples from 2 different authors, identified only by an unique id number (734562 and 449628);\n",
    "2. A new text whose author you need to identify between those two authors\n",
    "\n",
    "Guideline for analysis:\n",
    "- Just use state of the art techniques to attribute authorship\n",
    "\n",
    "Author's id 734562 samples:\n",
    "{author_samples[author_ids[0]]}\n",
    "\n",
    "Author's id 449628 samples:\n",
    "{author_samples[author_ids[1]]}\n",
    "\n",
    "New text to analyze:\n",
    "{test_text}\n",
    "\n",
    "Give as your answer only the author's id number and nothing else, like so: NNNNNN. Do not include any other text.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a completion function that will be used to query the model\n",
    "def get_completion(prompt: str, max_tokens=5000) -> str:\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734562\n"
     ]
    }
   ],
   "source": [
    "first_prediction = get_completion(prompt)\n",
    "print(first_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: 734562\n",
      "Predicted author: 734562\n",
      "Prediction is correct: True\n",
      "Accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "# Check if the prediction is accurate\n",
    "# Get the actual author of the random tweet\n",
    "actual_author = str(test_author_id).lower()\n",
    "\n",
    "# Get the predicted author from the model's response\n",
    "predicted_author = first_prediction.strip().lower()\n",
    "\n",
    "# Check if the prediction matches the actual author\n",
    "is_correct = predicted_author == actual_author\n",
    "\n",
    "print(f\"Actual author: {actual_author}\")\n",
    "print(f\"Predicted author: {predicted_author}\")\n",
    "print(f\"Prediction is correct: {is_correct}\")\n",
    "\n",
    "# Calculate accuracy (1 for correct, 0 for incorrect)\n",
    "accuracy = 1 if is_correct else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample dataset with 100 blog posts\n",
      "Sample distribution:\n",
      "id\n",
      "449628    60\n",
      "734562    40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample 100 test blog posts for evaluation\n",
    "n_samples = 100\n",
    "sampled_posts = df_test.sample(n=n_samples, random_state=42).reset_index(drop=True)\n",
    "print(f'Created sample dataset with {len(sampled_posts)} blog posts')\n",
    "print('Sample distribution:')\n",
    "print(sampled_posts['id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_author_id(author_id):\n",
    "    return str(author_id).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of 100 blog posts in 10 batches\n",
      "Completed batch 1/10 (posts 0-9)\n",
      "  ⚠️  9 errors in this batch\n",
      "  Taking a 30 second break...\n",
      "Completed batch 2/10 (posts 10-19)\n",
      "  ⚠️  7 errors in this batch\n",
      "  Taking a 30 second break...\n",
      "Completed batch 3/10 (posts 20-29)\n",
      "  ⚠️  7 errors in this batch\n",
      "  Taking a 30 second break...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m         batch_errors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m         errors\u001b[38;5;241m.\u001b[39mappend((idx, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_prediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 10\n",
    "total_batches = len(sampled_posts) // batch_size + (1 if len(sampled_posts) % batch_size > 0 else 0)\n",
    "\n",
    "results = []  # (idx, text, actual_author, predicted_author)\n",
    "errors = []\n",
    "print(f'Starting processing of {len(sampled_posts)} blog posts in {total_batches} batches')\n",
    "\n",
    "for batch in range(total_batches):\n",
    "    start_idx = batch * batch_size\n",
    "    end_idx = min((batch + 1) * batch_size, len(sampled_posts))\n",
    "    batch_errors = 0\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        row = sampled_posts.iloc[idx]\n",
    "        blog_text = row['text']\n",
    "        actual_author = normalize_author_id(row['id'])\n",
    "        prompt = f\"\"\"\n",
    "You are an expert in linguistic analysis and authorship attribution. Your task is to analyze authorship--i.e. idiolectal--markers that the texts reveal about their authors.\n",
    "\n",
    "You will be provided with:\n",
    "1. Training samples from 2 different authors, identified only by an unique id number (734562 and 449628);\n",
    "2. A new text whose author you need to identify between those two authors\n",
    "\n",
    "Guideline for analysis:\n",
    "- Just use state of the art techniques to attribute authorship\n",
    "\n",
    "Author's id 734562 samples:\n",
    "{author_samples[author_ids[0]]}\n",
    "\n",
    "Author's id 449628 samples:\n",
    "{author_samples[author_ids[1]]}\n",
    "\n",
    "New text to analyze:\n",
    "{blog_text}\n",
    "\n",
    "Give as your answer only the author's id number and nothing else, like so: NNNNNN. Do not include any other text.\n",
    "\"\"\"\n",
    "        max_retries = 3\n",
    "        retry_delay = 120\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                raw_prediction = get_completion(prompt).strip()\n",
    "                prediction = normalize_author_id(raw_prediction)\n",
    "                if prediction in [normalize_author_id(author_ids[0]), normalize_author_id(author_ids[1])]:\n",
    "                    results.append((idx, blog_text, actual_author, prediction))\n",
    "                else:\n",
    "                    batch_errors += 1\n",
    "                    errors.append((idx, f'Invalid prediction: {raw_prediction}'))\n",
    "                time.sleep(10)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if retry < max_retries - 1:\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    batch_errors += 1\n",
    "                    errors.append((idx, str(e)))\n",
    "                    break\n",
    "    print(f'Completed batch {batch + 1}/{total_batches} (posts {start_idx}-{end_idx-1})')\n",
    "    if batch_errors > 0:\n",
    "        print(f'  ⚠️  {batch_errors} errors in this batch')\n",
    "    if batch < total_batches - 1:\n",
    "        print('  Taking a 30 second break...')\n",
    "        time.sleep(30)\n",
    "\n",
    "print('\\nProcessing complete!')\n",
    "print(f'Successfully processed: {len(results)} blog posts')\n",
    "if errors:\n",
    "    print(f'Total errors: {len(errors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if results:\n",
    "    predicted_authors = [r[3] for r in results]\n",
    "    actual_authors = [r[2] for r in results]\n",
    "    correct = sum(p == a for p, a in zip(predicted_authors, actual_authors))\n",
    "    final_accuracy = correct / len(predicted_authors)\n",
    "    print(f'Overall accuracy: {final_accuracy:.2%} ({correct}/{len(predicted_authors)})')\n",
    "    # Per-author accuracy\n",
    "    for author in [normalize_author_id(author_ids[0]), normalize_author_id(author_ids[1])]:\n",
    "        author_cases = [(p == author) for _, _, a, p in results if a == author]\n",
    "        if author_cases:\n",
    "            author_accuracy = sum(author_cases) / len(author_cases)\n",
    "            print(f'Accuracy for {author}: {author_accuracy:.2%} ({sum(author_cases)}/{len(author_cases)})')\n",
    "    # Confusion matrix\n",
    "    valid_results = [(a, p) for _, _, a, p in results if p in [normalize_author_id(author_ids[0]), normalize_author_id(author_ids[1])]]\n",
    "    if valid_results:\n",
    "        y_true = [r[0] for r in valid_results]\n",
    "        y_pred = [r[1] for r in valid_results]\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[normalize_author_id(author_ids[0]), normalize_author_id(author_ids[1])])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=[str(author_ids[0]), str(author_ids[1])], yticklabels=[str(author_ids[0]), str(author_ids[1])])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "    # Qualitative examples\n",
    "    results_df = pd.DataFrame(results, columns=['idx', 'text', 'actual_author', 'predicted_author'])\n",
    "    correct_cases = results_df[results_df['actual_author'] == results_df['predicted_author']]\n",
    "    incorrect_cases = results_df[results_df['actual_author'] != results_df['predicted_author']]\n",
    "    n_samples = 10\n",
    "    sampled_correct = correct_cases.sample(n=min(n_samples, len(correct_cases)), random_state=42)\n",
    "    sampled_incorrect = incorrect_cases.sample(n=min(n_samples, len(incorrect_cases)), random_state=42)\n",
    "    print('SOME CORRECTLY CLASSIFIED EXAMPLES:\\\\n')\n",
    "    for _, row in sampled_correct.iterrows():\n",
    "        print(f'Blog: {row[\\\"text\\\"][:100]}...\\\\nActual: {row[\\\"actual_author\\\"]} | Predicted: {row[\\\"predicted_author\\\"]}\\\\n{\\\"-\\\"*60}')\n",
    "    print('\\\\nSOME INCORRECTLY CLASSIFIED EXAMPLES:\\\\n')\n",
    "    for _, row in sampled_incorrect.iterrows():\n",
    "        print(f'Blog: {row[\\\"text\\\"][:100]}...\\\\nActual: {row[\\\"actual_author\\\"]} | Predicted: {row[\\\"predicted_author\\\"]}\\\\n{\\\"-\\\"*60}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
